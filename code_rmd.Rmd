---
title: "The Spotify Project"
author: "Arnob Chanda, Kai Mei, Hanying Chen"
date: "11/10/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Intro here

## EDA and cleaning

### General Cleaning

```{r}
df <- read.csv("top10s.csv")
summary(df)
```

First column 'X' is redundant so removing it. Also take a look at the summary statistics.

```{r}
df <- subset(df, select = -X)
summary(df)
```

Checking if any title is repeated.

```{r}
unique_titles <- unique(df[c("title")])
nrow(unique_titles)
```

We have 584 unique values, out of 603 total values. So we have 19 repeated titles.

```{r}
n_title_repeat <- data.frame(table(df$title))
n_title_repeat[n_title_repeat$Freq > 1, ]
```

Checking if everything is the same in the repeated titles by creating a data frame with only the repeated values, sorting it and checking what is different.

```{r}
repeated_titles <- df[df$title %in% n_title_repeat$Var1[n_title_repeat$Freq > 1], ]
library(dplyr)
repeated_titles_sorted <- arrange(repeated_titles, title)
head(repeated_titles_sorted)
```

Looking at the data, we can see that there are some songs, the only difference is the year.
For others, the artist name is different. 
If the only difference is the year, we remove the second occurrence of it, else we keep the row.

```{r}
#Start with Repeated column set to FALSE
df$Repeated = FALSE
#Iterate over all the rows in the data frame
for(i in 1:nrow(df))
{
  #Get the indexes of the repeated values
  same_title_index = which(df$title == df$title[i])
  if(length(same_title_index)>1)
  {
    #The first occurrence is same_title_index[1], second one is same_title_index[2]
    #Check artist name for both indexes and if its the same artist name mark the second occurrence as TRUE
    if(df[same_title_index[1],]$artist == df[same_title_index[2],]$artist)
    {
      df[same_title_index[2],]$Repeated = TRUE
    }
  }
}

summary(df)
```

So, out of the 19 titles having the same names, 16 of them have the same artist name.
The second occurrence of the same title and artist has been marked TRUE.
Based on this, we can remove those repeated rows and make a cleaner data frame.

```{r}
rows_to_be_deleted <- which(df$Repeated == TRUE)
df_clean <- df[-c(rows_to_be_deleted),]
df_clean <- subset(df_clean, select = -Repeated)
summary(df_clean)
```

### Check for missing data

From the summary we can see that there are no missing data.
Plotting the missing data to confirm.

```{r}
# install.packages('VIM')
library(VIM, quietly = TRUE)
aggr(df_clean)
```

As there is no missing data, we don't need to replace any rows or columns.

### Check for outliers

Using Mahal score to eliminate outliers.

```{r}
df_variables <- df_clean[,-c(1,2,3,4)]
mahal <- mahalanobis(df_variables,
                     colMeans(df_variables),
                     cov(df_variables), use = "pairwise.complete.obs")
summary(mahal)
```

```{r}
cutoff <- qchisq(1-.001, ncol(df_variables))
cutoff
```
The cutoff value of Mahal distance is 29.5883
```{r}
summary(mahal < cutoff)
```

So we have 20 outliers.
Removing it.

```{r}
noout <- subset(df_clean, mahal<cutoff)
summary(noout)
```

### Check Correlation
```{r cor}
library(corrplot, quietly = TRUE)
corrplot(cor(noout[-c(1,2,3,4)]))
```

Based on the correlation plot above, we can see `acous` has a strong negative correlation with `nrgy` while `nrgy` and `dB` are strongly negatively correlated.

### Visualize the distribution of `top.genre` and `year`
```{r}
library(ggplot2)
cleanup <- theme(panel.grid.major = element_blank(),
                panel.grid.minor = element_blank(),
                panel.background = element_blank(),
                axis.line.x = element_line(color = 'black'),
                axis.line.y = element_line(color = 'black'),
                legend.key = element_rect(fill = 'white'),
                text = element_text(size = 12))

genre_freq <- as.data.frame(table(df$top.genre))
year_freq <- as.data.frame(table(df$year))

bar_lot_year <- ggplot(data = year_freq, aes(x = Var1, y = Freq, fill = Var1))
bar_lot_year + 
   stat_summary(fun = mean,
                 geom = 'bar',
                 position = 'dodge',
                 ) +
    xlab("Year") +
    ylab("Count") + 
    labs(title="Number of songs by year") +
    cleanup

# We have too many genres in the dataset, so we only view the top 5 genres and group the rest into 'Others'
k <- 5
top_k_genres <- genre_freq[order(-genre_freq$Freq), ][1:k, ]
others <- subset(genre_freq, !(Var1 %in% top_k_genres$Var1))
df_others <-data.frame("others", sum(others$Freq))
names(df_others)<-c("Var1", "Freq")
# combine two dfs
df_new <- rbind(top_k_genres, df_others)

pie_genre <- ggplot(data = df_new, aes(x = "", y = Freq, fill = Var1))
pie_genre + 
    geom_col() +
    coord_polar(theta = "y") +
    scale_fill_brewer(palette = "RdYlBu") +
    guides(fill = guide_legend(title = "Genre")) +
    ggtitle("Most popular genres")+
    theme_void()
```

From year 2010 to 2019, year 2015 has the largest number of songs. The top 5 popular genres are dance pop, pop, canadian pop, barbadian pop and boy band.

### Popular artists
```{r}
k <- 10
artists <- as.data.frame(table(noout$artist))
top_k_artists <- artists[order(-artists$Freq), ][1:k, ]
top_k_artists
```

### Check Assumptions
1. Additivity
```{r additivity}
# TODO: Add interpretation
model <- lm(pop ~., data = noout[-c(1,2,3,4)])  # do not include title, artist, genre or year
summary(model, correlation = TRUE)
```

2. Linearity
```{r linearity}
# TODO: Add interpretation
standardized <- rstudent(model)
fitted <- scale(model$fitted.values)
{qqnorm(standardized)
abline(0,1)}
```

3. Normality
```{r normality}
hist(standardized, breaks=15)
```
The histogram of standardized residuals is left skewed, indicating the normality assumption might be invalid.

4. Homogeneity and Homoscedasticity
```{r homogs}
fitvalues <- scale(model$fitted.values)
{plot(fitvalues, standardized) 
abline(0,0)
abline(v = 0)}
```
Homogeneity assumption does not hold well since the spread below 0 is slightly wider than above, while homoscedasticity holds given that the spread across x-axis is generally even.

## Modeling
### Build Full Model:

```{r}
full.model <- lm(pop ~., data = noout[-c(1,2,3,4)])
summary(full.model)
anova(full.model)
```

### Build Model based on high correlation (nrgy, live, dur, dnce, acous)

```{r}
model_high_corr <- lm(pop ~ nrgy + live + dur + dnce + acous, data = noout)
summary(model_high_corr)
anova(full.model,model_high_corr)
```

### Build Stepwise Model:

```{r}
#install.packages('leaps')
#install.packages('MASS')
library(MASS)
library(leaps)
step.model <- stepAIC(full.model, direction = "both",trace = FALSE)
summary(step.model)
anova(full.model,model_high_corr,step.model)
```

## Model Evaluation

Visualize the fitted values vs actual.

```{r}
{plot(full.model$fitted.values,
     noout$pop,
     xlab = "Predicted Values",
     ylab = "Actual Values")
abline(a = 0,
       b = 1,
       col = "red",
       lwd = 2)}

{plot(step.model$fitted.values,
     noout$pop,
     xlab = "Predicted Values",
     ylab = "Actual Values")
abline(a = 0,
       b = 1,
       col = "red",
       lwd = 2)}
```

Find the best model for predicting popularity. We will use root mean squared error to evaluate model performance.

```{r}
# install.packages('Metrics')
library(Metrics)
rmse(noout$pop, full.model$fitted.values)
rmse(noout$pop, model_high_corr$fitted.values)
rmse(noout$pop, step.model$fitted.values)
```
